{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing PySpark Application\n",
    "This guide is a reference for writing robust tests for PySpark code.\n",
    "\n",
    "To view the docs for PySpark test utils, see here. To see the code for PySpark built-in test utils, check out the Spark repository here. To see the JIRA board tickets for the PySpark test framework, see here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a PySpark Application\n",
    "\n",
    "Here is an example for how to start a PySpark application. Feel free to skip to the next section, “Testing your PySpark Application,” if you already have an application you’re ready to test.\n",
    "\n",
    "First, start your Spark Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/03 05:59:56 WARN Utils: Your hostname, codespaces-e1276e resolves to a loopback address: 127.0.0.1; using 10.0.0.128 instead (on interface eth0)\n",
      "25/01/03 05:59:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/03 05:59:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create spark session\n",
    "spark = SparkSession.builder.appName(\"Testing_PySpark\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [{\"name\": \"John    D.\", \"age\": 30},\n",
    "  {\"name\": \"Alice   G.\", \"age\": 25},\n",
    "  {\"name\": \"Bob  T.\", \"age\": 35},\n",
    "  {\"name\": \"Eve   A.\", \"age\": 28}]\n",
    "\n",
    "df = spark.createDataFrame(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|age|    name|\n",
      "+---+--------+\n",
      "| 30| John D.|\n",
      "| 25|Alice G.|\n",
      "| 35|  Bob T.|\n",
      "| 28|  Eve A.|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "#remove additional space in name\n",
    "def remove_extra_space(df, column_name):\n",
    "    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "transformed_df = remove_extra_space(df, \"name\")\n",
    "\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Time!!\n",
    "\n",
    "Now, let's test our PySpark transformation function.\n",
    "\n",
    "One approach is to visually inspect the resulting DataFrame. However, this can be impractical for large DataFrames or input sizes.\n",
    "\n",
    "A better approach is to write tests. Below are some examples of how we can test our code, applicable for Spark 3.5 and above versions.\n",
    "\n",
    "Note that these examples are not exhaustive, as there are many other test frameworks you can use instead of unittest or pytest. The built-in PySpark testing utility functions are standalone, meaning they are compatible with any test framework or CI test pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option 1: Using Only PySpark Built-in Test Utility Functions\n",
    "\n",
    "For simple ad-hoc validation cases, you can use PySpark testing utilities like assertDataFrameEqual and assertSchemaEqual in a standalone context. This allows you to easily test PySpark code in a notebook session. For instance, if you want to assert equality between two DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark.testing\n",
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "\n",
    "# Example 1\n",
    "df1 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\n",
    "df2 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\n",
    "assertDataFrameEqual(df1, df2)  # pass, DataFrames are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "df1 = spark.createDataFrame(data=[(\"1\", 0.1), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\n",
    "df2 = spark.createDataFrame(data=[(\"1\", 0.109), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\n",
    "assertDataFrameEqual(df1, df2, rtol=1e-1)  # pass, DataFrames are approx equal by rtol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.testing.utils import assertSchemaEqual\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, DoubleType\n",
    "\n",
    "s1 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\n",
    "s2 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\n",
    "\n",
    "assertSchemaEqual(s1, s2)  # pass, schemas are identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Using Unit Test\n",
    "For more complex testing scenarios, you may want to use a testing framework.\n",
    "\n",
    "One of the most popular testing framework options is unit tests. Let’s walk through how you can use the built-in Python unittest library to write PySpark tests. For more information about the unittest library, see here: https://docs.python.org/3/library/unittest.html.\n",
    "\n",
    "First, you will need a Spark session. You can use the @classmethod decorator from the unittest package to take care of setting up and tearing down a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class PySparkTestCase(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        cls.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "\n",
    "class TestTranformation(PySparkTestCase):\n",
    "    def test_single_space(self):\n",
    "        sample_data = [{\"name\": \"John    D.\", \"age\": 30},\n",
    "                       {\"name\": \"Alice   G.\", \"age\": 25},\n",
    "                       {\"name\": \"Bob  T.\", \"age\": 35},\n",
    "                       {\"name\": \"Eve   A.\", \"age\": 28}]\n",
    "\n",
    "        # Create a Spark DataFrame\n",
    "        original_df = spark.createDataFrame(sample_data)\n",
    "\n",
    "        # Apply the transformation function from before\n",
    "        transformed_df = remove_extra_spaces(original_df, \"name\")\n",
    "\n",
    "        expected_data = [{\"name\": \"John D.\", \"age\": 30},\n",
    "        {\"name\": \"Alice G.\", \"age\": 25},\n",
    "        {\"name\": \"Bob T.\", \"age\": 35},\n",
    "        {\"name\": \"Eve A.\", \"age\": 28}]\n",
    "\n",
    "        expected_df = spark.createDataFrame(expected_data)\n",
    "\n",
    "        assertDataFrameEqual(transformed_df, expected_df)\n",
    "\n",
    "\n",
    "# When you run your test file with the pytest command, it will pick up all functions that have their name beginning with “test.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/03 06:00:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# pkg/etl.py\n",
    "import unittest\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate()\n",
    "\n",
    "sample_data = [{\"name\": \"John    D.\", \"age\": 30},\n",
    "  {\"name\": \"Alice   G.\", \"age\": 25},\n",
    "  {\"name\": \"Bob  T.\", \"age\": 35},\n",
    "  {\"name\": \"Eve   A.\", \"age\": 28}]\n",
    "\n",
    "df = spark.createDataFrame(sample_data)\n",
    "\n",
    "# Define DataFrame transformation function\n",
    "def remove_extra_spaces(df, column_name):\n",
    "    # Remove extra spaces from the specified column using regexp_replace\n",
    "    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkg/test_etl.py\n",
    "import unittest\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define unit test base class\n",
    "class PySparkTestCase(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.spark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate()\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        cls.spark.stop()\n",
    "\n",
    "# Define unit test\n",
    "class TestTranformation(PySparkTestCase):\n",
    "    def test_single_space(self):\n",
    "        sample_data = [{\"name\": \"John    D.\", \"age\": 30},\n",
    "                        {\"name\": \"Alice   G.\", \"age\": 25},\n",
    "                        {\"name\": \"Bob  T.\", \"age\": 35},\n",
    "                        {\"name\": \"Eve   A.\", \"age\": 28}]\n",
    "\n",
    "        # Create a Spark DataFrame\n",
    "        original_df = spark.createDataFrame(sample_data)\n",
    "\n",
    "        # Apply the transformation function from before\n",
    "        transformed_df = remove_extra_spaces(original_df, \"name\")\n",
    "\n",
    "        expected_data = [{\"name\": \"John D.\", \"age\": 30},\n",
    "        {\"name\": \"Alice G.\", \"age\": 25},\n",
    "        {\"name\": \"Bob T.\", \"age\": 35},\n",
    "        {\"name\": \"Eve A.\", \"age\": 28}]\n",
    "\n",
    "        expected_df = spark.createDataFrame(expected_data)\n",
    "\n",
    "        assertDataFrameEqual(transformed_df, expected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/socket.py:778: ResourceWarning: unclosed <socket.socket fd=75, family=2, type=1, proto=6, laddr=('127.0.0.1', 46764), raddr=('127.0.0.1', 43647)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/python/3.12.1/lib/python3.12/socket.py:778: ResourceWarning: unclosed <socket.socket fd=75, family=2, type=1, proto=6, laddr=('127.0.0.1', 48312), raddr=('127.0.0.1', 40739)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.551s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x704b0dae3380>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=0, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
